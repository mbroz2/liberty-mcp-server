# MCP Server - remote
quarkus.langchain4j.mcp.weather.transport-type=streamable-http
quarkus.langchain4j.mcp.weather.url=http://localhost:9080/mcp-liberty-server/mcp

quarkus.http.port=8080
%dev.quarkus.http.port=8080

# Debug
quarkus.langchain4j.log-requests=true

# Ollama settings
langchain4j-ollama-dev-service.ollama.host=localhost
langchain4j-ollama-dev-service.ollama.port=11434
langchain4j-ollama-dev-service.ollama.endpoint=http://${langchain4j-ollama-dev-service.ollama.host}:${langchain4j-ollama-dev-service.ollama.port}
quarkus.langchain4j.chat-model.provider=ollama
quarkus.langchain4j.embedding-model.provider=ollama

# Ollama LLM Models
quarkus.langchain4j.ollama.chat-model.model-id=gpt-oss:20b

# Model hyperparameters
quarkus.langchain4j.ollama.chat-model.temperature=.6
quarkus.langchain4j.ollama.chat-model.top-p=.6
quarkus.langchain4j.ollama.chat-model.top-k=30

# Timeouts
quarkus.langchain4j.ollama.timeout=120s
quarkus.langchain4j.timeout=120s
quarkus.vertx.max-worker-execute-time=120s